\begin{frame}{Preliminary on DQN \cite{DBLP:journals/corr/MnihKSGAWR13}}
\begin{enumerate}
    \item In contrast to TD-Gammon and similar approaches, a technique known as experience replay is used, where the agent’s experiences are stored at each time-step, $e_t = (s_t, a_t, r_t, s_{t+1})$ in a data-set $\mathcal{D} = e_1, \dots, e_N$ , pooled over many episodes into a replay memory.
    \item Q-learning updates, or minibatch updates are applied to samples of experience, $e \in \mathcal{D}$, drawn at random from the pool of stored samples.
    \begin{center}
        $L(\theta) = \mathbb{E} \limits_{(s,a)}\Big[(y_j − Q(s , a ; \theta))^2\Big]$\\
        $\theta &= \theta - \eta \nabla L(\theta)$
    \end{center}
    \item After performing experience replay, the agent selects and executes an action according to an $\epsilon$-greedy policy.
    \begin{center}
        With probability $\epsilon$ select a random action $a_t$\\
        otherwise select $a_t = \max_a Q(s_t, a; \theta)$
    \end{center}
    \item Learning from the randomized samples of experience replay memory reduces the correlations among samples and therefore reduces the variance of the updates.
\end{enumerate}
\end{frame}

\begin{frame}{DQN: Algo}
\begin{algorithm}[H]
\caption{Deep Q-learning with Experience Replay}
	\begin{algorithmic}[1]
	    \State Initialize replay memory $\mathcal{D}$ to capacity $N$;
	    \State Initialize action-value function $Q$ with random weights;
		\For {$episode=1,2,\ldots, M$}
		    \State Initialise sequence $s_1 = \{x_1\}$, preprocess sequence $\phi_1 = \phi(s_1)$
		    \For {$t=1,2,\ldots, T$} 
		    \State With probability $\epsilon$ select a random action $a_t$
		    \State otherwise select $a_t = \max_a Q(s_t, a; θ)$ 
		    \State Execute action $a_t$ in emulator and observe reward $r_t$ and $x_{t+1}$
		    \State Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$
		    \State Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in $\mathcal{D}$
		    \State Sample random minibatch of transitions $(\phi_j , a_j , r_j , \phi_{j+1}) \in \mathcal{D}$
		    \State Set $y_j = r_j$ \quad for terminal $\phi_{j+1}$
		    \State Set $y_j = r_j + \max_{a'} Q(\phi_{j+1}, a'; \theta)$ \quad for non-terminal $\phi_{j+1}$
		    \State Perform a gradient descent step on $(y_j − Q(\phi_j , a_j ; \theta))^2$
		    \EndFor
		\EndFor
	\end{algorithmic} 
\end{algorithm}
\end{frame}